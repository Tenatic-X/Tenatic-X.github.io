---
layout: post
title: "Week 20&21"
description: "Week 20&21"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 20 & 21

## Relevant Work
[04 Transfer Learning with TensorFlow Part 1: Feature Extraction](https://tenatic-x.github.io/projects/(study)%2004_transfer_learning_with_tensorflow_part1.html)

## Overview
Continuing on tensorflow course work 05, and finishing up workbook 04 homework.

## Project and Tasks
* Completing 04 homework exercises
* Completing 05 - transfer learning part 2
* Completing 05 homework

## Challenges and Learnings

### Augmentation sequential model
To implement augmentation onto training data, we can create it as a small sequential model, stating the type of augmentation such as rotation, stretching, flipping etc.

### Models as layers
The cool thing is that, you can treat models as layers with `.layers`. So for example, the augmentation model above, can be setup as a layer, and be placed at a specific location that suits your needs. Below is an idea of what you can do with models, and layering it up. Such as using a `base_model` that's

```python
# Create input layer
inputs = layers.Input(shape=input_shape, name="input_layer")

# Add in data augmentation Sequential model as a layer
x = data_augmentation(inputs)

# Give base_model inputs (after augmentation) and don't train it
x = base_model(x, training=False)

# Pool output features of base model
x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)

# Put a dense layer on as the output
outputs = layers.Dense(10, activation="softmax", name="output_layer")(x)

# Make a model with inputs and outputs
model_1 = keras.Model(inputs, outputs)
```

## Next steps
Finish up the last bit of 04 homework exercise, by using images ive taken of 2 different objects, and use transfer learning on a pretrained model. Also start work on the 05 notebook.

## Reflection
I realize there is many downtime during work. And being in front of a computer all day, it was really convenient to study and work on data science when there are no customers to deal with. So I've been trying to utilize the time in between customers to be productive and study. I'm writing this blog post while I'm at work lol.
