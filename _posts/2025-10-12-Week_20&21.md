---
layout: post
title: "Week 20&21"
description: "Week 20&21"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 20 & 21

## Relevant Work
[04 Homework of Transfer Learning Part 1](https://tenatic-x.github.io/projects/(homework)%2004_exercise_transfer_learning_part_1.html)

[05 Transfer Learning with TensorFlow Part 2: Fine-tuning](https://tenatic-x.github.io/projects/(study)%2005_transfer_learning_with_tensorflow_part_2.html)

[05 Homework of Transfer Learning Part 2](https://tenatic-x.github.io/projects/(homework)%2005_exercise_transfer_learning_with_tensorflow_part_2.html)

## Overview
Continuing on tensorflow course work 05, and finishing up workbook 04 homework.

## Project and Tasks
* Completing 04 homework exercises
* Completing 05 - transfer learning part 2
* Completing 05 homework

## Challenges and Learnings

### Augmentation sequential model
To implement augmentation onto training data, we can create it as a small sequential model, stating the type of augmentation such as rotation, stretching, flipping etc.

### Models as layers
The cool thing is that, you can treat models as layers with `.layers`. So for example, the augmentation model above, can be setup as a layer, and be placed at a specific location that suits your needs. Below is an idea of what you can do with models, and layering it up. Such as using a `base_model`, where you are doing transfer learning.

```python
# Create input layer
inputs = layers.Input(shape=input_shape, name="input_layer")

# Add in data augmentation Sequential model as a layer
x = data_augmentation(inputs)

# Give base_model inputs (after augmentation) and don't train it
x = base_model(x, training=False)

# Pool output features of base model
x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)

# Put a dense layer on as the output
outputs = layers.Dense(10, activation="softmax", name="output_layer")(x)

# Make a model with inputs and outputs
model_1 = keras.Model(inputs, outputs)
```
The following code at the bottom, is how the model will look like. You can have models within models!
```python
Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_layer (InputLayer)    [(None, 224, 224, 3)]     0         
                                                                 
 data_augmentation (Sequenti  (None, None, None, 3)    0         
 al)                                                             
                                                                 
 efficientnetv2-b0 (Function  (None, None, None, 1280)  5919312  
 al)                                                             
                                                                 
 global_average_pooling_laye  (None, 1280)             0         
 r (GlobalAveragePooling2D)                                      
                                                                 
 output_layer (Dense)        (None, 10)                12810     
                                                                 
=================================================================
Total params: 5,932,122
Trainable params: 12,810
Non-trainable params: 5,919,312
_________________________________________________________________
```

Once that's created, you then compile the model with it's loss, optimizer and metrics.

```python
# Compile the model
model_1.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=["accuracy"])
```

**NOTE**: If you make any changes to the model, such as unfreezing layers in the base model to do `fine tuning`, you will need to do `compile` on the model again. Think of it as like you're saving the model with 'ctrl+s'.

### How to experiment with models
The advice given, is to follow where your curiousity goes. If you think you would like to try a model over another for transfer learning, or change the amount of strength in a weight, etc. You should try it. While experimenting, you should keep it short, often 5 epochs, and/or 10% training data. So you don't waste too much time in the in-between, when waiting for the model to train.

### Creating model checkpoints with tf callback
An awesome thing with tensorflow callback, is creating model checkpoints. If there is a need to save the weights of your model at it's current state, you can do so and save a file to use it for later. 

### Training via fine-tuned model
When training on a transferred model, but wanting to fine-tune it, you allow training on the top of the layers first. AKa, the layers at the end of the sequence of layers. As these are the layers that are the last ones before the model predicts it, based on their outputs. Learning rates are tuned down, so the weights are not heavily afflicted from its original. As the weights that are currently in use, are usually very optimized already. We're nudging it closer to it's trough, based on the problem it has recieved.

## Next steps
Continue to finish up the workbook 05 homework.

## Reflection
Not much, except busy with 6 day work weeks. Already at streak number 9, of weeks with working 6 days lol. So not a whole lot of down time to practice and study.
