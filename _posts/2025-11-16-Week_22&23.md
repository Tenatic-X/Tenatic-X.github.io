---
layout: post
title: "Week 22&23"
description: "Week 22&23"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 22 & 23

## Overview
Continuing on tensorflow course work 05, and finishing up workbook 04 homework.

## Project and Tasks
* Reading Ulmfit text classification
* Continuing 06 - scaling up part 3

## Challenges and Learnings

### Sciency jargon from Ulmfit science paper
Its hard for me to stay focused on reading a wall of text, and it was another challenge for data sciency heavy lingo lol. Luckily having Chatgpt summarizing every chapter, in addition to me regurgitating what I was reading back to Chatgpt to correct me, helped me understand it in better context.

### How to get a language model good at a task specific problem
Paper describes how difficult it is to get the model to learn the patterns of English, no less on context and prediction of things, whether it be sentiment, or predicted review rating. However there's a 3 step process to breach that gap.

1. Train the model on a huge bank of english data that can be used as a general starting point. E.g. using wikipedia
2. Fine tune the model using data that represents the problem, you are targetting so the model understands the specificity on the type of context, English is playing. E.g. Imdb reviews
3. Fine tune again, but with an extra input layer that starts predicting the problem with a few labeled data to learn from. E.g. learning what makes a 1 star review or 10 star in Imdb

### The ulmfit traingle
Normal finetuning, where unfreezing a number of layers

### Training via fine-tuned model
When training on a transferred model, but wanting to fine-tune it, you allow training on the top of the layers first. AKa, the layers at the end of the sequence of layers. As these are the layers that are the last ones before the model predicts it, based on their outputs. Learning rates are tuned down, so the weights are not heavily afflicted from its original. As the weights that are currently in use, are usually very optimized already. We're nudging it closer to it's trough, based on the problem it has recieved.

## Next steps
Continue to finish up the workbook 05 homework.

## Reflection
Not much, except busy with 6 day work weeks. Already at streak number 9, of weeks with working 6 days lol. So not a whole lot of down time to practice and study.
