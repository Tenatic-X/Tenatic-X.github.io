---
layout: post
title: "Week 22&23"
description: "Week 22&23"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 22 & 23

## Overview
Continuing on tensorflow course work 05, and finishing up workbook 04 homework.

## Project and Tasks
* Reading Ulmfit text classification
* Continuing 06 - scaling up part 3

## Challenges and Learnings

### Sciency jargon from Ulmfit science paper
Its hard for me to stay focused on reading a wall of text, and it was another challenge for data sciency heavy lingo lol. Luckily having Chatgpt summarizing every chapter, in addition to me regurgitating what I was reading back to Chatgpt to correct me, helped me understand it in better context.

### How to get a language model good at a task specific problem
Paper describes how difficult it is to get the model to learn the patterns of English, no less on context and prediction of things, whether it be sentiment, or predicted review rating. However there's a 3 step process to breach that gap.

1. Train the model on a huge bank of english data that can be used as a general starting point. E.g. using wikipedia
2. Fine tune the model using data that represents the problem, you are targetting so the model understands the specificity on the type of context, English is playing. E.g. Imdb reviews
3. Fine tune again, but with an extra input layer that starts predicting the problem with a few labeled data to learn from. E.g. learning what makes a 1 star review or 10 star in Imdb

### The ulmfit traingle
Normal finetuning, where unfreezing a number of layers then train again won't suffice. This causes too much forgetting of previous knowledge in English because learning rate is too high overall. To navigate this issue, when starting a `lr` of a layer, we would increase it quickly for a number of iterations, before gradually decreasing `lr` back, to kind of let the model weights settle slowly. This creates a slanted triangle shop for the `lr` overtime, during the iterations/batches the model goes through. 

On the next epoch, the model goes down a layer, but `lr` is reduced by `/2.6`, as deeper the layer you go, the more general knowledge the layer has about english. Meaning higher `lr` to these layers will cause the model to easily forget English. While upper layers have very specific knowledge, and is therefore not heavily afflicted from higher `lr. Again, these layers perform the slanted triangle shape of `lr`, with top layer at `x` of `lr`, and second layer at `x/2.6` of `lr`. And so on.

### Tensortflow unable to use GPU
I spent a good few hours to prepare my computer for gpu use on tensorflow. Getting the right version of python, tensorflow, nvidia software etc. but was never able to make it to work. Conversely to pytorch, it was able to use my gpu without issue right out of the box, rather than tensorflow where you one specific version, is only able to run on very specific software version. 

After tensorflow, I most likely will use pytorch for the forseeable future, only because i can't gpu to work on it, and the time difference in model training is extremely drastic.

### Always try visualize model prediction rather than numbers
You can only learn and ascertain so much with numbers alone. If you really want to understand the crux of the issues or further analyze/become one with the data, its always good to have multiple visual representation of the model prediction with it's true class/label.

For example with our food dataset, we can visually see why a model predicted wrong because two dishes look very similar to each other. Or there's mislabeling/errors in our test data.

## Next steps
Continue to finish up the workbook 06.

## Reflection
N/A, just been a bit slow with the study sessions, and spending a lot of downtime at work, trying to understand ulmfit: https://arxiv.org/pdf/1801.06146
