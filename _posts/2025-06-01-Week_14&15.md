---
layout: post
title: "Week 14&15"
description: "Week 14&15"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 14 & 15

## Relevant Project
[03_convolutional_neural_networks_in_tensorflow](https://tenatic-x.github.io/projects/(study)%2003_convolutional_neural_networks_in_tensorflow.html)

## Overview
Continuing tensorflow project from '03 notebook', teaching CNNs (Convolutional Neural Network) and Computer Vision

## Project and Tasks
* Study and learn from notebook 03

## Challenges and Learnings
If I were honest about the challenges now being about 90% complete with the notebook, there weren't too many so far. A lot of the things that have been discussed in the notebook, was knowledge that I've familiarized myself from the self teaching in NZMSA's poorly written learning materials, and also the trials and errors that occured from my own ML project/previous works.

But it was good to still keep the knowledge fresh in my mind, from data augmentation, and data pooling, to overfitting and batches. However there are still a few thing's I've learnt.

### Smaller the batch size the better
The notebook teaches how when finding a batch size to use when training a model, smaller batches work better than bigger ones. As GPU's memory may not fit onto so many images at once, and trying to learn patterns with a giant pool of data is not the most efficient. There's some experimentation that's done on optimal batch sizes, and shows how small ones tend to be preferred.

### CNN structure
Input > Conv + ReLU layers (non-linear activations) > Pooling layer > Fully connected (Dense layer) as output

There's actually a clear structure or flow direction for a CNN structured layer. When I was doing my first ML project in weight and waist prediction, I just trialed and errored where the layers go, and only had a marginal idea of how these layers are supposed to be organized and distributed. Knowing the way they are placed, and why they are placed there due to what each layer does, will help me save time during experimentation.

### Make model overfit, then reduce it
A thing the notebook teaches, is when doing model training for the first time, make the model overfit. It gives us an understanding of whether the ML model has the capability of learning the patterns of the dataset, to the point where it sees patterns where there isn't in the training dataset. If it's not doing so, then either the model is too simple, or more data is required to learn these patterns.

Once overfitting is performed, we can slowly back away from a super complex model that overfits, to a semi complex model that generalizes the data better.

![DbvTRbkX0AA2fRh](https://github.com/user-attachments/assets/9ce01e9e-a2c3-4149-866f-62e86ee809ae)

## Next steps

FInish up workbook 03, then continue on with some of the homework and extra-cirriculars on the bottom.

## Reflection

I'm a bit happy that despite not being able to recall the details of machine learning and the steps that is involved, I'm still able to recognize them from the texts, and don't need too much refreshers to understand it's purpose.
