---
layout: post
title: "Week 29&30"
description: "Week 29&30"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 29&30

## Relevant work
[08. Natural Language Processing with TensorFlow (homework)](https://tenatic-x.github.io/projects/(homework)%2008_Natural_Language_Processing_with_TensorFlow.html)

## Overview
Completing 08 homework notebook.

## Project and Tasks
* Continuing onto 09 workbook, milestone NLP project.

## Challenges and Learnings

### Professionality is important

I've begin to start looking for junior or graduate role in data science, and have been researching and consulting chatgpt on how to prepare and make/include things that make me seem employable. One thing I learnt is professionalism being very important.

Because I chose this space as archiving and recollection of my own work, I also wanted to archive my personal feelings, including anger and frustration through obscene words - but because of the inclusion of these blog posts, and ipynb notebook, recruiters can read my work and conclude that the communication done here, is what would be represented in the workplace.

I use those languages to make it more vibrant and colourful, because it can feel very monotonous writing these sometimes sadly, but have made an explanation note, that this space is for my own thoughts and ideas, and not generally used as full purpose following materials (which presentation is obviously different).

One very important aspect is the pfp I previously used. A stalin head, superimposed on a big breast model, in front of a red background with the hammer and sickle communist sign. Very sexual, political, and inappropriate - I've gone ahead and changed that to something goofy and can't be interpretted as something fringe and far. Obviously that old pfp pic will illicit very negative reactions.

A little unfortunate these spaces have to maintain professionality, but I understand it's important to be that way, and not exhibit antisocial behaviour and continuity.

### More TensorFlow and Keras incompatability issues

An exercise relies on me to fine tune the `USE` model. But my pc is not good enough, and tf is not able to recognize my gpu. So onwards to Google Colab, which unfortunately doesn't even work because of some sort of 'TensorFlow mismatch to keras???'

Anyways, I was able to run it but the paramaeters should `0` for the `USE` layer. Therefore the benefits of transfer learning isn't even functional. Prediction results were trash, and because of the sunk cost fallacy, have moved to the next step.

### Majority vote ensemble did surprisingly well compared to averaging predictions.

Average predictions is done by adding all prediction probabilities, then dividing by the number of models used > `[0.9,0.1,0.6] = 1.6 / 3 = 0.53333 = 1`

Majority predictions is done by rounding the prediction probabilities to 0 and 1, then pick the one that gets the most votes > `[0.9,0.1,0.6] > [1,0,1] > 1`

Predictions for Average Predictions:
```python
{'accuracy': 79.26509186351706,
 'precision': 0.7945205385015361,
 'recall': 0.7926509186351706,
 'f1': 0.7903183243034217}
```

Then majority vote:

```python
{'accuracy': 83.85826771653542,
 'precision': 0.8433806807504004,
 'recall': 0.8385826771653543,
 'f1': 0.8363842362800337}
```

### What is TF-IDF?

TF-IDF assesses how often a word appears in the single line, and also overall dataset, giving two separate data values that is combined together to give a value.

```python
TF–IDF Scenarios (per word, per document)

1) LOW TF × LOW IDF  → VERY LOW TF-IDF
   - Word appears rarely in this abstract
   - Word appears in many documents across the corpus
   - Example: stopwords or filler terms ('the', 'a', 'or')
   - Interpretation: not important to this document

2) HIGH TF × LOW IDF → LOW TF-IDF
   - Word appears frequently in this abstract
   - Word appears in many documents across the corpus
   - Example: domain-common words (e.g. "study", "results", "variable" in research papers)
   - Interpretation: common but not discriminative

3) LOW TF × HIGH IDF → MEDIUM TF-IDF
   - Word appears rarely in this abstract
   - Word appears in very few documents across the corpus
   - Example: rare but incidental terms (e.g. "scrumptious", "homosexual")
   - Interpretation: distinctive but not central

4) HIGH TF × HIGH IDF → HIGH TF-IDF
   - Word appears frequently in this abstract
   - Word appears in very few documents across the corpus
   - Example: document-specific keywords
   - Interpretation: highly informative and discriminative
```

* TF asks: "Is this word important to THIS abstract?"
* IDF asks: "Is this word unique compared to OTHER abstracts in the entirety of data?"
* TF-IDF rewards words that answer "yes" to both.

> Note: In practice, IDF (looking at word uniquness amongst whole dataset) often has more weight than TF (looking at abstract only for such words), because of it's broad reach on all data, rather than single isolated sections.

### Combining token and character level embedding in a model

So theres two ways to create embeddings. One token level (aka each word), and another character level. (each letter). Both can give representations and relationships in their own way, and what better way than to combine the two together to get their advantages?

With functional API model, we can `Concatenate` a character based model, and a token based model together (after they've passed through their vector and embed layers, as they are essentially two different datasets representing the same sequence).

The model perform and generalize well on the validation dataset. It also helped the model had 2 different `dropout()` layers, likely suggesting the combination of the 2 models, allowed the model to overfit too much.

This is also the visual order of the model, to allow a better understanding of how its architecture is formed:

<img width="775" height="995" alt="f2b1f6c9-3cf6-4de4-95c0-fdc404467b15" src="https://github.com/user-attachments/assets/e076b330-1bca-4594-acfa-ae8cf8a51548" />

## Next steps
Still continuing through 09 notebook.

## Reflection
Struggling to keep attention for long enough, cuz I end up having afternoon naps on most times I study past an hour or two lol.
