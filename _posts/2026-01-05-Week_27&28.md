---
layout: post
title: "Week 27&28"
description: "Week 27&28"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 27&28

## Relevant work
[08. Natural Language Processing with TensorFlow (study)](https://tenatic-x.github.io/projects/(study)%2008_Natural_Language_Processing_with_TensorFlow.html)

## Overview
Completed 08 notebook on NLP

## Project and Tasks
* Continuing on 08 workbook homework and reading materials.

## Challenges and Learnings

### Text vectorization - max vocab length use

When you vectorize texts, you need to havea set length for every sample that's provided. It's kind of like models, only accepting a certain shape or dimension of data, because it has only been trained on that shape or dimension.

Going by the longest sample, leaves too much 'whitespace 0s' and is very inefficient/time consuming. Too short, or the model does not recieve enough context in each data. The good middle ground is by finding the average number of text per data. Aka, all words, divided by all samples of data.

### Multinomial Naive Bayes model

At it's core for multinomial NB, is counting the number of times certain words appear on each labeled classes. E.g. The model may find `fruit` appear more often when it's in the category of `healthy`, rather than `asteroid`. 

This means the model doesn't identify context, grammar, meaning, etc. Just word choices, aka which is why it's named `Naive`. This makes the model great as a spam detector for words `free` or `win`, or binary classifiers.

### Have baseline model for experiments

To streamline multiple model experiments, it's a good idea to use a comparison model that is very simple in architecture. This gives you a good idea of how well your model has done in accuracy, comparitively to a simple model.

### Recurrent Neural Networks

The basics of a RNN (Recurrent Neural Network), is using information in the past to help you with the future. This is often useful with sequences of data, like a passage of text. There are also a variety of use cases for sequence-based problems:

* **One to one**: one input, one output, such as image classification.
* **One to many**: one input, many outputs, such as image captioning (image input, a sequence of text as caption output).
* **Many to one**: many inputs, one output, such as text classification (classifying a tweet as real disaster or not real disaster).
* **Many to many**: many inputs, many outputs, such as machine translation (English to Spanish), or speech to text (audio wave as input, text as output).

### Convolutional Neural Networks but for text

We've actually familiarized ourselves with CNNs before in image. It's much the same way with text but in 1-dimensional space. You would have a filter that spans a number of words, thena number of different filters that are applied to the same span of words.

For example:
```python
    # Our filter size = 3, and no. of filters = 2
    # `The cat sat on the mat` - Our text
    # Filter 1
    Window 1: ["The","cat","sat"] → 0.16
    Window 2: ["cat","sat","on"] → 0.12
    Window 3: ["sat","on","the"] → 0.18
    Window 4: ["on","the","mat"] → 0.14
    → Filter 1 output: [0.16, 0.12, 0.18, 0.14]

    # Filter 2
    Window 1: ["The","cat","sat"] → 0.11
    Window 2: ["cat","sat","on"] → 0.13
    Window 3: ["sat","on","the"] → 0.15
    Window 4: ["on","the","mat"] → 0.12
    → Filter 2 output: [0.11, 0.13, 0.15, 0.12]
```

Once we got our output, we would max pool them, and get a 1 dimensional vector, thats as long as the number of filters used on the text. (In this case, 2 for our example above).

## Next steps
I am up to homework of 08, but there's a lot more reading materials of 08, so may also try to get through some reading materials.

## Reflection
Still trying to pace well with workload, and make studying more frictionless for my mind.
