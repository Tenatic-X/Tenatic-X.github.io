---
layout: post
title: "Week 18&19"
description: "Week 18&19"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 18 & 19

## Relevant Work
[04 Transfer Learning with TensorFlow Part 1: Feature Extraction](https://tenatic-x.github.io/projects/(study)%2004_transfer_learning_with_tensorflow_part1.html)

## Overview
Continuing on tensorflow course work 04, and reading up on extra cirricular materials from past work books as well.

## Project and Tasks
* Completing 04 - tensorflow part 1 of feature extraction
* Completing 04 homework exercises
* Reading/Watching materials of workbook 03, on `CNNs` (Convolutional Neural Networks)
* Watching materials of workbook 02, on `Gradient Descent`

## Challenges and Learnings

### Struggle of using GPU on Tensorflow

GPUs are much more efficient at training computer models when compared to CPU, but however, it seems to always be a pain in the ass to actually implement it. I've struggled to get it to work on the tensorflow version I work with, because it doesn't fit with the GPU software version, or doesn't fit with my python version. So I'll be leaving it for another day, and just train with CPU to avoid the sunk cost fallacy.

### Training with 10% of data, on pretrained models

I've done some transfer knowledge model/work with the nzmsa program the past month, so was familiar with its concepts. However, it was still surprising how good the model's accuracy was, despite the test folder, having more pictures than the train folder.

### Variations of transfer learning on models

There are namely 3 different ways you can perform transfer learning:
1. **As is** > The pretrained model remains unchanged when using on your problem. This is done if your problem, is almost if not exactly the same as the problem the model was trained on.
2. **Feature extraction** > Only the top layer of the model is changed. This is when you have similar problems to the model that was trained on, but with different categories and variations of the image. Such as changing from 1000 food categories, to 20 dessert categories.
3. **Fine tuning** > Training some or all layers of the model. This is done by unfreezing the next 2-3 layers from the top, and slowly work your way down. This is done if the problem has some difference to the original problem the model is trained on. Like the model is trained for cats and dogs, but you want it to also recognize chinchillas.

<img width="1324" height="744" alt="68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6d7264626f75726b652f74656e736f72666c6f772d646565702d6c6561726e696e672f6d61696e2f696d616765732f30342d646966666572656e742d6b696e64732d6f662d7" src="https://github.com/user-attachments/assets/2cd60022-9320-42c7-8987-f689de480590" />
*Here's the relevant image to the 3 ways of transfer learning.*

### Tensorflow Callbacks

Very important with model training, when you want to compare different types of models, and how well they train on the same problem. They help log the information as a file on your pc, create model checkpoints, so you can train the model on different times, without needing to restart the entire training process due to timeout or pc power off, or early stopping if the model sees no improvement.

This is the relevant code to create a tensorflow call back function:
```python
import datetime
def create_tensorboard_callback(dir_name, experiment_name):
  log_dir = dir_name + "/" + experiment_name + "/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  print(f"Saving TensorBoard log files to: {log_dir}")
  return tensorboard_callback
```

### Transposed Convolution Arithmetic

You know how pooling in CNN, you use a filter with size 2x2, 3x3 etc. put it over the image, doing the mathematical operation over the filter and the original image pixels, then you get the new value, and move a certain amount of pixels, aka strides? You are able to reverse the filtering, by using the same filter to expand it back up.

Caveat, is when expanding, if you used padding (the zeroes that surround the image to maintain image size or align the output of the filter, so it fits on the whole image), your transposed version won't be padded. Vice versa if no padding is used when filters applied to shrink image.

<img width="508" height="229" alt="erfbebrerb" src="https://github.com/user-attachments/assets/cc646553-2431-48fe-a0c7-4362332a1b60" />

As long as you remember the kernel shape and the values in each pixel, you can bring back to the original image.

### Dilated Convolution
To save even more resources during CNNs, you can do something, where either the filters or the image itself has adjacent blank pixels. You can still maintain the image's appearance and it's size, while doing training on it at a much rapid pace, since there is much less to calculate

<img width="464" height="124" alt="bgtfrhthh5" src="https://github.com/user-attachments/assets/857aaccf-d48a-4fdd-b0d7-d7c9df8bc98b" />
<img width="475" height="133" alt="gggrswgswrrggr" src="https://github.com/user-attachments/assets/f3639c1b-e4a5-47b0-a256-1b98172124f0" />

The above is examples of dilation.

### Gradient Descent
When model training, the model would randomly output prediction probabilities. The loss is then calculated against the true label of say, an image. The more different the prediction is to the true label, the higher the loss is.

This loss is then communicated to the model, how that is bad, and the number must be decreased. The model will then change it's weight of the neurons, towards the direction where loss will decrease. Rinse and repeat, until we find the lowest loss.

An analogy, is you have a plain of hills. The height of the hills represent the loss. Higer the hills, higher the loss. You have a ball that's affected by gravity, which represents the model. The ball rolls in whichever direction that pulls them down the quickest, and would roll until it reaches a trough, aka the model that produces the smallest loss.

<img width="1008" height="377" alt="berfdtebtr" src="https://github.com/user-attachments/assets/14965e78-0e02-4101-a5cd-92cfe76c643f" />

If you look at the image example, there are other troughs, but are separated from others with hills. Sometimes the model will get better or worse results, depending on their starting point on the hills. They can't magically go over hills to a lower trough, as the only form of momentum is the gravity that pulls them down.

### Data science lingo

With all these reading materials, I really struggled with the lingo and terms that were usd on the workbooks, papers, and videos. With a simple copy paste on chatgpt, I was able to understand them. And it turns out, a lot of the concepts and information were things I was already familiar with. They were just put into data science terms, which made the concepts seem almost foreign, despite how simple an analogy can clear things up/using graphs and images.

## Next steps
Finish up the last bit of 04 homework exercise, by using images ive taken of 2 different objects, and use transfer learning on a pretrained model. Also start work on the 05 notebook.

## Reflection
I realize there is many downtime during work. And being in front of a computer all day, it was really convenient to study and work on data science when there are no customers to deal with. So I've been trying to utilize the time in between customers to be productive and study. I'm writing this blog post while I'm at work lol.
