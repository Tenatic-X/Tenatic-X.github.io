---
layout: post
title: "Week 10"
description: "Week 10"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 10

## Relevant Project
[02_homework](https://tenatic-x.github.io/_projects/02_homework.html)

## Overview
Finishing up the exercises/homework of notebook 02

## Project and Tasks
* Complete 02 workbook: *We just learn the materials set out in 02, mostly on binary and multi classification, but also how we can find the best learning rate*

## Challenges and Learnings

### Crescent dataset
We do a lot of similar code contents from the circle separation exercise. But this time, we're differentiating two different coloured crescent shapes, over two ovals. Everything was familiar with the code and the steps to take between each snippet of code. Only thing unfamiliar was the dataset, which didn't take long to understand, since I made sure to visualize it, and understand it through my eyes.

### Creating softmax function
Had to re-create what the softmax function does, which means I had to learn what every bit of softmax actually does. On paper, it sounded easy to me. As it was identifying an image, by using probability between all categories, then picking the one with the highest probability. Obviously it wasn't as simple as it seems on the surface. So what does it **actually do?**

  1. **Input** > We get an array of raw scores, aka our tensors, into our softmax function `[2.0, 1.0, -2.0]`.
  2. **Exponentiation** > Apply exponent, using the **Euler's number**, which has a value of roughly ~2.718. Weird ass value, but helps to accentuate probability values that are already high, but also removes negative probability.
     * `Exponents - Max raw score` is done to keep stability, by making sure no value become so out of proporion. Its like normalization in a way.
     * `Euler's Number` > [e^2.0, e^1.0, e^-2.0] = [7.39, 2.71, 0.135] Again, Euler's number is ~2.718. All the raw scores, become the exponent of Euler's number. And Euler's number being positive, keeps all probability values positive as well.
  3. **Normalization** > Sum up all values from step 2, then divide each category probability, with the sum. This gives the total category a sum of 1, equating to 100%. Aka, for every image with a list of all categorical probabilities, we end up with a comprehensive list of the percentage that the image is likely to be this specific category.
  4. **Output** > Shat out the probability list of the categories per image.

It was definitely more complex than what I initially thought!

### Achieve >88% accuracy on the fashion MNIST test dataset.

This wasn't any different than some of the previous workbooks. Where I trial and errored through different ML model training methods. Here's a list of things I've tried:
1. Normalized training data by dividing with `255.0` > *as models tend to do better with values ranging from 0 to 1*
2. Finding ideal learning rate of `Adam` > *to more effeciently find where the model learns best, not too strong, not too weak*
3. Added `Convolutional Layer` > *allows me to give specified commands on how the model learns image patterns/features*
4. Added `Pooling layer` > *reduces image size, but keeps most important features of images, which helps reduce small noise/dust particles*
5. Added `Batch Normalization` > *prevents covariate shift, aka prevents too much distribution/shifting in training, leading to wasted time and energy on correcting small differences/avoids vanishing and exploding gradients*
6. Increase Convolutional Layer's `filter` to `32` > *allows convolutional layer to learn even more different feature maps of the image*
7. Increase `relu` layer 1, to `32`, then layer 2 to `16` > *creating a sort of funnelling down effect, where the neuron below is all connected to at least 2 other neurons above it*

### Create function to show a certain class from `fashion MNIST`, and its prediction of it
I've had a lot of trouble trying to figure out what was wrong with my function, and have consulted ChatGPT to try and fix my broken code, but couldn't find a way to fix it :( So I've abandoned my last exercise, to focus my energy on other projects, rather than wasting away at one problem that I couldn't get right.

## Next steps

Make a start again on the weight and waist prediction dataset. Now that I've learnt tensorflow, it's a very good arsenal to figure out how to best make the ML model predict my weight and waist difference, over simple models, with a limited number of options on hyperparameter tweaking.

## Reflection

Never underestimate a pre-built function! Sometimes, things sound simple by word of mouth, but is a whole other beast when you have to figure out all the tiny loopholes and errors in the data!
