---
layout: post
title: "Week 11"
description: "Week 11"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 11

## Relevant Project
[NZMSA content](https://github.com/NZMSA/2025-Phase-1)

## Overview
Attempting the New Zealand Microsoft Accelerator programme, and ideating my next data science/ml engineering project in stocks.

## Project and Tasks
* Finish up Phase 1 learning materials of NZMSA
* Start working out the kinks of this stock data science project, and what's nessecary to make it come to life.

## Challenges and Learnings

### Repeated content of NZMSA
I've done the NZMSA programme from last year in addition to writing and commenting on the code that was presented in the github repo. And for the most part, it's 95% of the same content I've already studied. Despite wanting to skip to only the parts I have not learned, I still tried to read through and skim the code a bit, despite my struggling ability to keep focus on repeated materials, or just reading in general.

### Getting bored of reading documentation
The absolute bane of my existince is sitting my ass down, and reading pages on documentation with paragraphs of words that could've been condensed into 3 fuckin sentences :( Really no forms of avoiding it but brute force reading the repo of NZMSA phase 1, making sure I still understand the stuff that I've done a year ago, or just move onto something else and give myself a break.

### What does a real data scientist do in their day-to-day jobs?
Finishing up my very first self-directed project made me question whether what I was doing is remotely close to what data scientists/ml engineer tackle with in their day-to-day careers? I've decided to upload my work to chatgpt and ask for its opinion, as I didn't believe my work is close to their complexity. So there are a few things to note from what ChatGPT said:
1. Problem definitions are often ambiguous and vague "what's causing low customer sales", rather than a straight to the point "predicting weight changes".
2. Messier, larger and less transparent data, with fragmented pieces across different servers, and missing, undocumented, and misleading files that aren't structured nicely on an excel sheet.
3. Scalability and constraining is important as resources are limited, and continuous retraining may be nessecary with many versions of data.
4. Become mindful of ethics, bias, and the real consequences of false postives or false negatives like predicting heart disease.
5. Monitoring performance overtime, as models may degrade and retraining may be nessecary.
6. Collaboration and communication is key with all forms of staff members and showing insights of your findings.
7. Tolling and infrastructure for cloud platforms, or automation and being able to reproduce results.

### Using S&P 500 Stocks to simulate data science/ML engineer workplace
With this stock project, we could simulate those 7 points for the most part, into our new project pipeline. A list of things could include:
* Instead of whether the stock will go up or down, the model bets a certain amount from our wallet, based on gathered information of s&p 500 values and related news articles
* Use multiple sources that track s&p 500 futures/a limit of how many times we can scrape the data
* Adding constraints to how many features we can have on the model, and whether misclassifying down-markets has a higher cost than up-markets
* Prediction is given by probability level of whether stock prices will increase or decrease
* Could update the model every week as it runs through a trial
* Running multiple variations of the stock trading, one where entire lump sum is bought, one where I plan the way I invest, and the one for the model itself, making the amount it wants to invest.

### Natural language processing models for S&P 500 and more
This project tackles a lot of new stuff I'm not familiar with. Not only do I create a model that predicts the porbability of the stock going up or down in the next 24 hour period, there is one NLP model (natural language processing) for curating relevant and current news articles, relevant to the S&P 500, but another NLP model that picks the sentiment of said currated news articles, and using the sentiment of the articles to predict the percentage of how much s&p 500 will change, increase or decrease in the next 24 hour period. Luckily NLP pretrained models are available for me to use and train it on these news articles, such as `finbert`, `all-MiniLM-L6-v2`, etc.

The first model for curating articles could be simply replaced by a keyword selector, where `S&P 500` or `Stocks` is mentioned, or also train based on the biggest headline within the past 24 hours, either USA centric or worldwide. Then pick only the top 100 articles for our sentiment model.

### Final equation to suggest buy or sell of S&P 500
There was a lot of consideration on how we can use the prediction of probability that the stocks will increase or decrease, and the prediction of the percentage change of the stocks in a 24 hour period to create a singular value that gives us the final number of how much the model wants to invest, or withdraw. Here are the list of considerations to this final equation:
1. It can handle 4 possible inputs (pos, pos) (neg, neg) (pos, neg) (neg, pos)
2. It can produce a final value out of the 4 inputs, where both positives and negatives accentuate each other, and the conflicting values reach a sort of middle ground of which side to favour, based on its predicted value
3. It can produce a value amount to invest based on what is currently in our wallet, or an amount to withdraw based on what has been invested, dictated by that singular value from 2
4. It can standardize the values of both probability and percentage prediction, so they are on a similar scale
5. It allows fine tuning on which prediction has more influence, in case one side is too strong during validation
6. It allows the adjustment of how aggressive the model should invest from wallet, or withdraw from investments

#### Variable glossary:
* $$p$$: Predicted probability of price increase/decrease (0.5 represents no change)
* $$r$$: Predicted stock's percentage change (0.01 represents 1%)
* $$σ_R$$: Standard Deviation of true percentage change in stocks (limit to moving average of 30 days)
* $$β$$: Control strength for $$σ_R$$ to ease the final value size after dividing $$r$$, due to volatility of stocks
* $$α$$: Control for the weight distribution between $$p_{std}$$ and $$r_{std}$$ (1 = p_{std} full control)
* $$λ$$: Control for how aggressive the model should invest or withdraw
* $$W$$: Current Wallet
* $$I$$: Current Investments

Let's establish random values to each variable name, so you can see them in action next to the equations:
* $$p$$: 0.65 (`Model 1 predicts a slight chance of stock increase in the next 24 hours`)
* $$r$$: -0.02 (`Model 2 predicts -2% decrease in stock price in the next 24 hours`)
* $$σ_R$$: 0.0085 (`30 day moving window of stock change's standard deviation`)
* $$β$$: 10 (`This should decrease the influence strength of r, so p will have a chance to influence as well`)
* $$α$$: 0.5 (`Split half half influence between r and p for now`)
* $$λ$$: 0.5 (`50% Aggression when investing or withdrawing, so any overflowed signal (e.g. >1) will still have valid values for us to take out of wallet/investments (up to value of 2 tho)`)
* $$W$$: 5000
* $$I$$: 1000

#### Standardization:
```math
p_{std} = 2(p - 0.5) \qquad p_{std} = 0.3 = 2(0.65 - 0.5)
```
```math
r_{std} = \frac{r}{σ_R ⋅ β} \qquad r_{std} = -0.235 = \frac{-0.02}{0.0085 ⋅ 10}
```
#### Investing Signal:
```math
signal = α ⋅ p_{std} + (1 - α) ⋅ r_{std} \qquad signal = 0.0325 = 0.5 ⋅ 0.3 + (1 - 0.5) ⋅ -0.235
```
#### Amount Buy:
```math
\text{Invest Amount} = λ ⋅ W ⋅ signal \qquad $81.25 = 0.5 ⋅ $5000 ⋅ 0.0325
```
#### Amount Sell:
```math
\text{Withdraw Amount} = λ ⋅ I ⋅ -signal \qquad -$16.25 = 0.5 ⋅ $1000 ⋅ -0.0325
```

So with our mock value in equation, we got two separate values. We will always ignore the negative values due to its invalidity. You can't withdraw $-16.25, unless we frame it as adding $16.25 more. This wouldn't work as we're using the investment value on our wallet value, which can cause extreme values as they are not proportionate to each other. (e.g. -$700 in `Withdraw Amount` when our `Wallet` is $50).

Big thing with this equation is needing to trial and error our assumed values of $$β$$, $$α$$, $$λ$$. Will have to play around the values during validation, since there's a bit or arbitration going on in the way I try to scale both probability predicting with percent change predicting, especially if both values are conflicting each other (one negative, one positive).

## Next steps

Continue studying the rest of NZMSA, while jotting any important notes for stocks personal project.

## Reflection

I like to think a lot and use my brain to theorize ways to solve my problems in the S&P 500 project that I have yet to create. Absolutely hate being brain dead and reading documentation with ineffecient phrasing to much word count in their essays.
