---
layout: post
title: "Week 11"
description: "Week 11"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 11

## Relevant Project
[NZMSA content](https://github.com/NZMSA/2025-Phase-1)

## Overview
Attempting the New Zealand Microsoft Accelerator programme, and ideating my next data science/ml engineering project in stocks.

## Project and Tasks
* Finish up Phase 1 learning materials of NZMSA
* Start working out the kinks of this stock data science project, and what's nessecary to make it come to life.

## Challenges and Learnings

### Repeated content of NZMSA
I've done the NZMSA programme from last year in addition to writing and commenting on the code that was presented in the github repo. And for the most part, it's 95% of the same content I've already studied. Despite wanting to skip to only the parts I have not learned, I still tried to read through and skim the code a bit, despite my struggling ability to keep focus on repeated materials, or just reading in general.

### Getting bored of reading documentation
The absolute bane of my existince is sitting my ass down, and reading pages on documentation with paragraphs of words that could've been condensed into 3 fuckin sentences :( Really no forms of avoiding it but brute force reading the repo of NZMSA phase 1, making sure I still understand the stuff that I've done a year ago, or just move onto something else and give myself a break.

### What does a real data scientist do in their day-to-day jobs?
Finishing up my very first self-directed project made me question whether what I was doing is remotely close to what data scientists/ml engineer tackle with in their day-to-day careers? I've decided to upload my work to chatgpt and ask for its opinion, as I didn't believe my work is close to their complexity. So there are a few things to note from what ChatGPT said:
1. Problem definitions are often ambiguous and vague "what's causing low customer sales", rather than a straight to the point "predicting weight changes".
2. Messier, larger and less transparent data, with fragmented pieces across different servers, and missing, undocumented, and misleading files that aren't structured nicely on an excel sheet.
3. Scalability and constraining is important as resources are limited, and continuous retraining may be nessecary with many versions of data.
4. Become mindful of ethics, bias, and the real consequences of false postives or false negatives like predicting heart disease.
5. Monitoring performance overtime, as models may degrade and retraining may be nessecary.
6. Collaboration and communication is key with all forms of staff members and showing insights of your findings.
7. Tolling and infrastructure for cloud platforms, or automation and being able to reproduce results.

### Using S&P 500 Stocks to simulate data science/ML engineer workplace
With this stock project, we could simulate those 7 points for the most part, into our new project pipeline. A list of things could include:
* Instead of whether the stock will go up or down, the model bets a certain amount from our wallet, based on gathered information of s&p 500 values and related news articles
* Use multiple sources that track s&p 500 futures/a limit of how many times we can scrape the data
* Adding constraints to how many features we can have on the model, and whether misclassifying down-markets has a higher cost than up-markets
* Prediction is given by probability level of whether stock prices will increase or decrease
* Could update the model every week as it runs through a trial
* Running multiple variations of the stock trading, one where entire lump sum is bought, one where I plan the way I invest, and the one for the model itself, making the amount it wants to invest.

### Natural language processing models for S&P 500 and more
This project tackles a lot of new stuff I'm not familiar with. Not only do I create a model that studies the stock values, there is one NLP model (natural language processing) for curating relevant and current news articles, relevant to the S&P 500, but another NLP model that picks the sentiment of said currated news articles.
  

## Next steps

Make a start again on the weight and waist prediction dataset. Now that I've learnt tensorflow, it's a very good arsenal to figure out how to best make the ML model predict my weight and waist difference, over simple models, with a limited number of options on hyperparameter tweaking.

## Reflection

Never underestimate a pre-built function! Sometimes, things sound simple by word of mouth, but is a whole other beast when you have to figure out all the tiny loopholes and errors in the data!
