---
layout: post
title: "Week 31&32"
description: "Week 31&32"
tags: Weekly(ish)_update
todolist: Title, Overview, Projects and Tasks, Challenges and Solutions, Learnings and Insights, Next Steps, Reflections
---

# Week 29&30

## Relevant work
[09. Milestone Project 2: SkimLit ğŸ“„ğŸ”¥ (study)](https://tenatic-x.github.io/projects/(study)%2009_milestone_project_2_skimlit.html)

## Overview
Completed 09 milestone project 2

## Project and Tasks
* Continuing onto 09 homework notebook.

## Challenges and Learnings

### Building a triple hybrid model using 'Words, Characters, and Positional embedding/info'

When observing how abstracts work, there tends to be a pattern on what type of information it is communiating. You would expect `Conclusion` near the end, than on the front. Which is why the notebook included positional information, as another data point for the model to learn from. Again, like chars and token numbers in an entire abstract, some abstracts may have lots of lines. So we would find the total lines that cover 95% of the abstract length as a middle ground between getting as much info as possible, while reducing heavy padding.

* `line_number` = the index number position of the line from the abstract
* `total_lines` = stating the total number of lines from the abstract

With those information combined, the model would know which line of the abstract it is seeing, and in addition, how far it is into the abstract (since abstracts have different total line lengths). Now combined with the token and character embedding architecture, this does make things more complex.

The model's creation is funneling/combining the `token` and `character` embedding models together. Then use hybrid model to combine with `line_number` and `total_line` models together to form something called a `tribrid`.

This is the following code:

```python
# 1. Token inputs
token_inputs = layers.Input(shape=[],dtype='string',name='token_inputs')
token_embeddings = tf_hub_embedding_layer(token_inputs)
token_outputs = layers.Dense(128,activation='relu')(token_embeddings)
token_model = tf.keras.Model(inputs=token_inputs,outputs=token_outputs)

# 2. Char inputs
char_inputs = layers.Input(shape=(1,),dtype='string',name='char_inputs')
char_vector = char_vectorizer(char_inputs)
char_embeddings = char_embed(char_vector)
char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)
char_model = tf.keras.Model(inputs=char_inputs,outputs=char_bi_lstm)

# 3. Line number inputs
line_number_inputs = layers.Input(shape=(15,),dtype=tf.int32,name='line_number_input')
x = layers.Dense(32,activation='relu')(line_number_inputs)
line_number_model = tf.keras.Model(inputs=line_number_inputs,outputs=x)

# 4. Total lines inputs
total_lines_inputs = layers.Input(shape=(20,),dtype=tf.int32,name='total_lines_input')
x = layers.Dense(32,activation='relu')(total_lines_inputs)
total_line_model = tf.keras.Model(inputs=total_lines_inputs,outputs=x)

# 5. Combine token and char embedding into hybrid
combined_embeddings = layers.Concatenate(name='token_char_hybrid_embedding')([token_model.output,
                                                                              char_model.output])
z = layers.Dense(256,activation='relu')(combined_embeddings)
z = layers.Dropout(0.5)(z)

# 6. Combine positional embeddings with hybrid embed, making tribrid embedding
z = layers.Concatenate(name='token_char_pos_embedding')([line_number_model.output,
                                                         total_line_model.output,
                                                         z])

# 7. Create output layer
output_layer = layers.Dense(5, activation='softmax', name='output_layer')(z)

# 8. Put together model
model_5 = tf.keras.Model(inputs=[line_number_model.input,
                                 total_line_model.input,
                                 token_model.input,
                                 char_model.input],
                                 outputs=output_layer)
```

And then the following architecture image:

<img width="968" height="839" alt="64fbe5de-4a97-43ab-abda-4ea6f7edd7ae" src="https://github.com/user-attachments/assets/05612b20-c51f-4e57-9f2c-0a53b434cc36" />

> **Why structure the models in this way?** - Good question, I don't know at this point as I just followed the instructions on the notebook. It could be because of the data type its working with, like the `strings` together and `integers` at another point? This may be a good place to revisit again and see what changes will it make to the performance.

### Keeping track of models within models

Naturally with the combination of 4 different models, operating and learning patterns on 4 different sets of data, it becomes quite difficult and overwhelming when trying to keep track of every piece, and not missing anything crucial.

Thing to always keep in mind for myself, is always printing out the `output` of something, to make sure I am dealing with the write contents and as a form of sanity check, when I get overwhelmed of information and contents. 

(But it also doesn't help that working with tensorflow, when converting it into `tf data`, you actually can't print it out into a human friendly readable format).

```python
# the kinda dumbass output tf datasets give - can't understand shit lmao
(<_PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>,
 <_PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>)
```

### I hate google colab's environment/versions

The tribrid model of `token/char/pos` had only been trained on 10% of the data, but the homework asks me to try full data instead.

```python
These were the results:
Epoch 1/3
562/562 [==============================] - 54s 82ms/step - loss: 1.1041 - accuracy: 0.7167 - val_loss: 0.9859 - val_accuracy: 0.8015
Epoch 2/3
562/562 [==============================] - 45s 79ms/step - loss: 0.9687 - accuracy: 0.8151 - val_loss: 0.9518 - val_accuracy: 0.8245
Epoch 3/3
562/562 [==============================] - 45s 80ms/step - loss: 0.9512 - accuracy: 0.8234 - val_loss: 0.9400 - val_accuracy: 0.8331
```

Again, I would've thought Google Colab would help drastically speed up training due to GPU access, contrasting me. Sadly life ain't that easy because of it's damn Tensorflow and Keras versions, having a tantrum over a **line** of code. Specifically `token_embeddings = tf_hub_embedding_layer(token_inputs)`.

Apparently, the `keras 3` version had shit with this code, and is deemed **illegal**. Some little loopholes were made to still get the embedding to our tokens, with the trade off on how long it took:

```python
Epoch 1/1000
5627/5627 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1357s 239ms/step - accuracy: 0.7820 - loss: 1.0113 - val_accuracy: 0.8475 - val_loss: 0.9084
Epoch 2/1000
5627/5627 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1341s 238ms/step - accuracy: 0.8510 - loss: 0.9143 - val_accuracy: 0.8536 - val_loss: 0.8991
Epoch 3/1000
5627/5627 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1342s 238ms/step - accuracy: 0.8600 - loss: 0.9012 - val_accuracy: 0.8544 - val_loss: 0.8975
Epoch 4/1000
5627/5627 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1342s 239ms/step - accuracy: 0.8671 - loss: 0.8921 - val_accuracy: 0.8550 - val_loss: 0.8971
Epoch 5/1000
5627/5627 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1354s 241ms/step - accuracy: 0.8720 - loss: 0.8853 - val_accuracy: 0.8551 - val_loss: 0.8973
Epoch 6/1000
5627/5627 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1375s 236ms/step - accuracy: 0.8761 - loss: 0.8790 - val_accuracy: 0.8544 - val_loss: 0.8973
Epoch 7/1000
5627/5627 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1374s 241ms/step - accuracy: 0.8797 - loss: 0.8738 - val_accuracy: 0.8535 - val_loss: 0.8987
```

Apparently the full dataset causes the training time per epoch to increase about **25x**? But the data is only 10 times the size as before! Obeviously there's more to figure out with Google Colab, but it sure was dissapointing the amount of time the model took to train for just 2% improvement in accuracy.

### Not all pretrained embeddings are `plug and play`.

This was a bit of a hard pill to swallow, as most if not all the stuff I've played with were as simple as copy and paste. Maybe change the input and output section of the transfered model when needed.

But as an embedding, some things work different than changing input and output shape to match the data we're having the model to process. We're working with human language, where each word has it's own specific meaning and referencing. But computers aren't able to understand this intuitively, so it uses numbers to associate and represent words. Especially such as representation of words.

Because there are thousands of words, they must compartmentalize to a value that represents it, aka vectorization through counting all unique words in the dataset. This creates mismatch with vectors of your dataset, to the pretrained model.

To give the step by step process of how we get the embed to work with our dataset.

**Step 1:** We must first import the file into a dictionary variable. Looping through every word in the file, and attaching it to it's embedding dimension, that has the representation of such word.

```python
for line in glove_path:
    values=line.split() # split lines on spaces to get words, or aka tokens
    word=values[0] # store word/token on the first column
    vector=np.asarray(values[1:],dtype='float32') # second column stores vector array
    embedding_index[word]=vector # adds the entries into the dictionary
```

now the results in the dict looks like:
```python
[('life',
  array([-6.455100e-02, -8.097700e-02, -2.443290e-01,  2.415760e-01,
          6.618000e-02, -1.438670e-01, -3.585540e-01, -3.409890e-01,
          1.414754e+00,  1.044830e-01, -6.342880e-01, -5.992820e-01,
         -2.162860e-01, -2.591070e-01,  2.905930e-01,  1.733030e-01,
          2.805100e-02,  1.756900e-02,  2.496360e-01, -2.846450e-01,
         -3.259600e-01, -3.878030e-01, -5.822260e-01, -4.915400e-02,
         -5.878140e-01, -1.753720e-01,  2.648320e-01,  2.625390e-01,
         -2.040020e-01, -6.979570e-01,  1.941240e-01, -7.650400e-02,
          1.574700e-02,  1.648400e-02,  2.866450e-01,  2.112070e-01,
         -2.043020e-01,  1.865620e-01,  2.532370e-01,  8.683000e-03,
          2.279750e-01,  6.053900e-02, -3.843050e-01, -5.753290e-01,...
```

**Step 2:** Next we get our vectorized text, aka the list of unique vocab from our dataset, plus it's index location, which represents the word's vector:

```python
vocab = text_vectorizer.get_vocabulary() # get vocab that's been vectored in our dataset
vocab_index = dict(zip(vocab,range(len(vocab)))) # combine index numbering with our vocab, and form a dictionary with it
```

Then you get the following results.

```python
 [('', 0),
  ('[UNK]', 1),
  ('the', 2),
  ('and', 3),
  ('of', 4),
  ('in', 5),
  ('to', 6),
  ('with', 7),
  ('a', 8),
  ('were', 9)])
```

**Step 3:** Find all embedding values that exist for the vocabs in the dataset. We can do this by creating a fresh matrix to store all vocab from the dataset, in addition to it's embedding from our transfer learning. First, we loop through our entire unique vocab list, and see if the word exists in our `pretrained embed`.

IF it exists, we append the `pretrained embed` into the new matrix, at the index location of the word it looked at. Aka if `line` is in index `94` AND `line` exists in our `pretrained embed`, parse our `pretrained embed` into index `94` inside our new matrix.

ELSE if it was not found in our `pretrained embed`,  we will skip it and leave the new matrix with padded zeroes for that specific word.

You get the following results.

```python
# one example of existing embed applied to our new matrix, and another without
array([[-6.89530000e-02, -3.22241992e-01,  8.78690034e-02,
         2.67576993e-01,  3.73694003e-01, -3.27832013e-01,
        -3.11556011e-01, -2.52599001e-01,  6.57710016e-01,
        -7.33344972e-01, -1.78443000e-01,  4.14981008e-01,
...
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,
       ...]])
```

## Next steps
Still continuing through 09 homework notebook.

## Reflection
Getting back on rubifen my friend slipped me, which has somewhat helped with planting my ass down. - Because of the slow progression, I've finally started to look at psychiatrists for ADHD diagnosis, and get medicated for it.
